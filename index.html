<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="icon" href="path/to/favicon_F.ico" type="./src/img/favicon.ico">

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
    integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300&display=swap" rel="stylesheet"> 

  
  <title>FLOAT</title>
  <style>
    *{
    font-family: 'Roboto', sans-serif;
    padding: 0;
    margin: 0;
    outline: none;
    }
    .video-container {
    display: flex;
    gap: 10px;
    justify-content: center;
  }

  .video-container video {
    width: 100%;
    max-width: 365px;
    height: auto;
  }

    .embed-responsive {
    position: relative;
    overflow: hidden;
    }

    .embed-responsive-item {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: contain;
    }
  
    video {
      max-width: 100%;
      height: auto;
      display: block;
      }

    .btn-primary {
      background-color: navy;
      border-color: navy;
      color: white;
    }
    .btn-primary:hover {
      background-color: #004080;
      border-color: #004080;
    }

    a {
      color: navy;
      text-decoration: none;
      padding: 5px 10px;
      border-radius: 5px;
    }
    a:hover {
      color: #004080;
    }

    .custom-control-prev {
      position: absolute;
      left: -250px; 
      top: 50%;  
      transform: translateY(-50%);
      z-index: 5; 
    }

    .custom-control-next {
      position: absolute;
      right: -250px; 
      top: 50%;  
      transform: translateY(-50%);
      z-index: 5;  
    }
    .carousel-item video {
      max-width: 100%;
      height: auto;
    }

    .carousel-control-prev {
    position: absolute;
    left: -50px; 
    top: 50%;   
    transform: translateY(-50%);
    z-index: 10;
    font-size: 30px;
    width: 60px;    
    height: 60px;   
    background-color: rgba(0, 0, 0, 0.5);
    color: white;    
    border-radius: 50%;  
    display: flex;
    justify-content: center;
    align-items: center;
  }

  .carousel-control-next {
    position: absolute;
    right: -50px; 
    top: 50%;    
    transform: translateY(-50%);
    z-index: 10;
    font-size: 30px;
    width: 60px;    
    height: 60px;   
    background-color: rgba(0, 0, 0, 0.5);
    color: white;     
    border-radius: 50%;
    display: flex;
    justify-content: center;
    align-items: center;
  }

  .video-fit {
    width: 100%;
    height: auto;
    max-height: calc(100vh - 100px); 
    object-fit: contain; 
  }

  .carousel-control-prev:hover, .carousel-control-next:hover {
    background-color: rgba(0, 0, 0, 0.8);
  }  
  </style>

</head>

<body>
  <div class="container">
    <br>
    <div style="text-align: center;">  
      <h1>FLOAT</h1>
      <h3> Generative Motion Latent Flow Matching for Audio-driven Talking Portrait</h3>
      <div style="margin-top: 15px;">
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://taekyungki.github.io" target="_blank">Taekyung Ki<sup>1</sup></a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://kevinmin95.github.io" target="_blank">Dongchan Min<sup>1</sup></a></span>
        <span style="margin-right: 15px; font-size: 1.3em;"><a href="https://www.aistudios.com/ko/home" target="_blank">Gyeongsu Chae<sup>2</sup></a></span>
      </div>
      <div style="margin-top: 15px;">
        <span style="margin-right: 20px; font-size: 1.2em;"><sup>1</sup>KAIST</span>
        <span style="margin-right: 20px; font-size: 1.2em;"><sup>2</sup>DeepBrain AI Inc.</span>
      </div>
      <div style="margin-top: 15px;">
        <span style="font-size: 1.4em; font-weight: bold; color: navy;">
          ICCV 2025
        </span>
      </div>
    </div>
    <div class="text-center" style="font-size: 1.5em; margin-top: 25px;">
      <a class="btn btn-primary btn-lg" target="_blank"
        href="https://arxiv.org/abs/2412.01064" role="button"
        style="margin-right: 10px; margin-bottom: 10px;">Paper</a>
      <a class="btn btn-primary btn-lg" target="_blank"
        href="" role="button"
        style="margin-right: 10px; margin-bottom: 10px;">Video</a>
      <a class="btn btn-primary btn-lg" target="_blank"
        href="https://github.com/deepbrainai-research/float" role="button"
        style="margin-bottom: 10px;">Code</a>
      <a class="btn btn-primary btn-lg" target="_blank"
        href="https://huggingface.co/papers/2412.01064" role="button"
        style="margin-left: 10px; margin-right: 10px; margin-bottom: 10px;">HuggingFace</a>
    </div>

    <div style="margin-top: 30px;">
      <h2 class="text-center">
        Abstract
      </h2>
      <img src="./src/img/float-abstract.png" alt="" style="display: block; margin: 10px auto; width: 90%;">
      <br>
      <p style="font-style: italic; margin-bottom: 5px;">
        With the rapid advancement of diffusion-based generative models, portrait image animation has achieved remarkable results.
        However, it still faces challenges in temporally consistent video generation and fast sampling due to its iterative sampling nature.
        This paper presents FLOAT, an audio-driven talking portrait video generation method based on flow matching generative model.
        Instead of a pixel-based latent space, we take advantage of a learned orthogonal motion latent space, enabling efficient generation and editing of temporally consistent motion.
        To achieve this, we introduce a transformer-based vector field predictor with an effective frame-wise conditioning mechanism.
        Additionally, our method supports speech-driven emotion enhancement, enabling a natural incorporation of expressive motions.
        Extensive experiments demonstrate that our method outperforms state-of-the-art audio-driven talking portrait methods in terms of visual quality, motion fidelity, and efficiency.
      </p>
      <p style="font-size: 1.2em; margin-top: 0px;">
        <span style="font-weight: bold;">TL;DR:</span> FLOAT is a flow matching based audio-driven talking portrait video generation method, which can enhance the speech-driven emotional motion.</h4>
      </p>
    </div>


    <div style="margin-top: 50px;">
      <div class="text-center">
        <h2>
          Method Overview
        </h2>
        <img src="./src/img/overview.png" width=100% class="img-fluid" alt="Responsive image">
      </div>
      <div style="margin-top: 35px;">
        <p>
          Audio-driven talking portrait aims to synthesize talking portrait videos using a single source portrait image and driving audio.
          FLOAT is built upon a motion latent auto-encoder that encodes the given portrait image into an <span class="text-danger">identity-motion</span> latent representation.
          We generate audio-conditioned talking portrait motion latents through the flow matching (with optimal transport trajectories).
          To enhace the naturalness of generated talking motion, we incorporate the speech-driven emotion labels (&#128512;), providing a natural approach of emotion-aware talking portrait motion generation. 
        </p>
      </div>
    </div>

    <div style="margin-top:50px;">
      <h2 class="text-center"> Results </h2>
      <br>
      <h4> Results with Out-of-distribution Datas</h4>
      <p> FLOAT can generate realistic talking portrait videos using OOD portrait images and audio. </p>
      
      <div class="video-container">
        <video controls>
          <source src="src/video/ood_musk.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <video controls>
          <source src="src/video/ood_chinese.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <video controls>
          <source src="src/video/ood_monarisa.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <br>
      <div class="video-container">
        <video controls>
          <source src="src/video/ood_hinton.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <video controls>
          <source src="src/video/ood_altman.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <video controls>
          <source src="src/video/ood_paint.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <br>
      <div class="video-container">
        <video controls>
          <source src="src/video/ood_feifei.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <video controls>
          <source src="src/video/ood_taylor.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <video controls>
          <source src="src/video/ood_paint_angy.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <br><br>
      <h4>Test Time Head Pose Editing using the Learned Orthonormal Basis</h4>
      <p> Since FLOAT is trained on the learned orthonormal motion latent space, we can explicitly compute the motion coefficients from the sampled motion latents.
        Given a sampled motion latent \( w_{r \to D'} = \) using the flow matching and the learned orthonormal basis \( V = \{\mathbf{v}_m\}_{m=1}^{M}\), 
        we can assume that there exist \(\{\lambda_m (D')\}_{m=1}^{M}\) such that \(w_{r \to D'} = \sum_{m=1}^{M} \lambda_m (D')\cdot\mathbf{v}_m\) and can compute these motion coefficients 
        \(\{\lambda_m (D')\}_{m=1}^{M}\) by
        \[ \langle w_{r\to D'}, ~\mathbf{v}_k \rangle = \langle \sum_{m=1}^{M} \lambda_m (D') \cdot \mathbf{v}_m, ~\mathbf{v}_k \rangle = \lambda_k (D'), \]
        as \(\langle \mathbf{v}_m, \mathbf{v}_k\rangle = \delta_{m,k}\) (\(\delta\) is Kronecker delta function).
        This allows us to explicitly edit head motions at test time (e.g., modifying the primary direction of head movement), as shown in the below (please refer to the paper for more details):
      </p>    
      <div class="row">

          <div class="col-md-12 col-sm-12 col-xs-12 gallery">
            <div style="position: relative; padding-top: 0; overflow: hidden;">
              <video controls style="display: block; width: 100%; height: auto;">
                <source src="src/video/lambda_manipulation_15.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>


      <br><br>
      <h4>Emotion Redirection</h4>
      <p> Since FLOAT is trained with speech-driven emotion labels, it can re-direct the emotion of the talking portrait during the inference phase.
      Specifically, we can manipulate the predicted speech-driven emotion label with a simple one-hot emotion label, which can be further refined through classifier-free vector fields.
      This enables users to refine emotion even when the driving speech conveys ambiguious or mixed emotions. </p>
    
      <div class="row">
          <div class="col-md-12 col-sm-12 col-xs-12 gallery">
            <div style="position: relative; padding-top: 0; overflow: hidden;">
              <video controls style="display: block; width: 100%; height: auto;">
                <source src="src/video/emotion_redirection_1.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <br>
            <div style="position: relative; padding-top: 0; overflow: hidden;">
              <video controls style="display: block; width: 100%; height: auto;">
                <source src="src/video/emotion_redirection_2.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>

      <br><br>
      <h4>Comparison with State-of-the-art Methods</h4>
      <p> We compare with state-of-the-art non-diffusion-based methods and diffusion-based methods.
        For non-diffusion-based methods, we choose SadTalker and EDTalk.
        For diffusion-based methods, we choose AniTalker, Hallo, and EchoMimic.</p>
          <div id="carouselExampleControls" class="carousel slide" data-ride="carousel">
            <div class="carousel-inner">
              <div class="carousel-item active">
                <video class="d-block w-100 video-fit" controls playsinline>
                  <source src="./src/video/sota_comparison_01.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>

              </div>
              <div class="carousel-item">
                <video class="d-block w-100 video-fit" controls playsinline >
                  <source src="./src/video/sota_comparison_02.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <div class="carousel-item">
                <video class="d-block w-100 video-fit" controls playsinline>
                  <source src="./src/video/sota_comparison_03.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>

              <div class="carousel-item">
                <video class="d-block w-100 video-fit" controls playsinline>
                  <source src="./src/video/sota_comparison_04.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>

              <div class="carousel-item">
                <video class="d-block w-100 video-fit" controls playsinline>
                  <source src="./src/video/sota_comparison_05.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>

            </div>
            <a class="carousel-control-prev" href="#carouselExampleControls" role="button" data-slide="prev">
              <span class="carousel-control-prev-icon" aria-hidden="true"></span>
              <span class="sr-only">Previous</span>
            </a>
            <a class="carousel-control-next" href="#carouselExampleControls" role="button" data-slide="next">
              <span class="carousel-control-next-icon" aria-hidden="true"></span>
              <span class="sr-only">Next</span>
            </a>
          </div>
          <br><br>
          <div id="carouselExampleControls2" class="carousel slide" data-ride="carousel">
            <p>We further compare our method with other diffusion-based approaches, EMO and VASA-1, using their demo videos since their official implementations are not publicly available.</p>
            <div class="carousel-inner">
          
              <div class="carousel-item active">
                <video class="d-block w-100 video-fit" controls playsinline>
                  <source src="./src/video/compare_vasa-1_01.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
          
              <div class="carousel-item">
                <video class="d-block w-100 video-fit" controls playsinline>
                  <source src="./src/video/compare_vasa-1_02.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
          
              <div class="carousel-item">
                <video class="d-block w-100 video-fit" controls playsinline>
                  <source src="./src/video/compare_emo_01.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
          
              <div class="carousel-item">
                <video class="d-block w-100 video-fit" controls playsinline>
                  <source src="./src/video/compare_emo_02.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
          
            </div>
            <a class="carousel-control-prev" href="#carouselExampleControls2" role="button" data-slide="prev">
              <span class="carousel-control-prev-icon" aria-hidden="true"></span>
              <span class="sr-only">Previous</span>
            </a>
            <a class="carousel-control-next" href="#carouselExampleControls2" role="button" data-slide="next">
              <span class="carousel-control-next-icon" aria-hidden="true"></span>
              <span class="sr-only">Next</span>
            </a>
          </div>

    
      <br>
      <br>
      <h4>Ablation Studies on Frame-wise AdaLN and Flow Matching</h4>
      <p> We conduct ablation studies on frame-wise AdaLN (and gating) and flow matching. For frame-wise AdaLN (and gating), we compare it with cross-attention mechanism,
          which is widely used in conditional generation. For flow matching, we compare it with two types of diffusion models (\(\epsilon\)-prediction and \(x_0\)-prediction).
          We observe that frame-wise AdaLN (and gating) can generate more diverse head motions than cross-attention.
          We also observe that flow mathcing can generate more temporally consistent videos with accurate lip-synchronization than diffusion models.
      </p>
      <div id="carouselExampleControls1" class="carousel slide" data-ride="carousel">
        <div class="carousel-inner">
          <div class="carousel-item active">
            <video class="d-block w-100 video-fit" controls playsinline>
              <source src="./src/video/ablation_phase2_architecture_1.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>

          </div>
          <div class="carousel-item">
            <video class="d-block w-100 video-fit" controls playsinline >
              <source src="./src/video/ablation_phase2_architecture_2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="carousel-item">
            <video class="d-block w-100 video-fit" controls playsinline>
              <source src="./src/video/ablation_phase2_architecture_3.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

          <div class="carousel-item">
            <video class="d-block w-100 video-fit" controls playsinline>
              <source src="./src/video/ablation_phase2_architecture_4.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>

        <a class="carousel-control-prev" href="#carouselExampleControls1" role="button" data-slide="prev">
          <span class="carousel-control-prev-icon" aria-hidden="true"></span>
          <span class="sr-only">Previous</span>
        </a>
        <a class="carousel-control-next" href="#carouselExampleControls1" role="button" data-slide="next">
          <span class="carousel-control-next-icon" aria-hidden="true"></span>
          <span class="sr-only">Next</span>
        </a>
      </div>

      <br><br>
      <h4> Different Number of Function Evaluations (NFEs)</h4>
      <p> The small number of function evaluations (NFEs) affects temporal consistency. This is because we generate the motion latents, not the content itself.
          FLOAT is capable of generating reasonable video results with approximately 10 NFEs. </p>
      <div id="carouselExampleControls2" class="carousel slide" data-ride="carousel">
        <div class="carousel-inner">
          <div class="carousel-item active">
            <video class="d-block w-100 video-fit" controls playsinline>
              <source src="./src/video/ablation_phase2_NFE_1.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

          <div class="carousel-item">
            <video class="d-block w-100 video-fit" controls playsinline >
              <source src="./src/video/ablation_phase2_NFE_2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>

        </div>
        <a class="carousel-control-prev" href="#carouselExampleControls2" role="button" data-slide="prev">
          <span class="carousel-control-prev-icon" aria-hidden="true"></span>
          <span class="sr-only">Previous</span>
        </a>
        <a class="carousel-control-next" href="#carouselExampleControls2" role="button" data-slide="next">
          <span class="carousel-control-next-icon" aria-hidden="true"></span>
          <span class="sr-only">Next</span>
        </a>
      </div>

      <br>
      <br>
      <h4>Emotion Guidance Scales</h4>
      <p> We can control the intensity of the emotion by adjusting the emotion guidance scale.
      Note that the predicted speech-driven emotion label is <b>Disgust</b> with a 99.99% probability. </p>
      <div class="row">
          <div class="col-md-12 col-sm-12 col-xs-12 gallery">
            <div style="position: relative; padding-top: 0; overflow: hidden;">
              <video controls style="display: block; width: 100%; height: auto;">
                <source src="src/video/ablation_phase2_emotion-guidance.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>

      <br>
      <br>
      <h4>Additional Driving Conditions</h4>
      <p> We also investigate another types of driving conditions, such as 3DMM head pose parameters, to improve the controllability and naturalness.
          Here, 3DPose, S2E, and I2E are 3DMM head pose parameters, Speech-to-emotion label, and Image-to-emotion label, resepctively.
      </p>
      <div class="row">
          <div class="col-md-12 col-sm-12 col-xs-12 gallery">
            <div style="position: relative; padding-top: 0; overflow: hidden;">
              <video controls style="display: block; width: 100%; height: auto;">
                <source src="src/video/driving_condition_01.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <br>
            <div style="position: relative; padding-top: 0; overflow: hidden;">
              <video controls style="display: block; width: 100%; height: auto;">
                <source src="src/video/driving_condition_02.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
        </div>

    <br>
    <br>
    <h4>Ablation Study on Facial Component Perceptual Loss in Phase 1</h4>
    <p> The proposed facial component perceptual loss for the motion latent auto-encoder
      significantly improves visual quality (e.g., teeth and eyes), as well as fine-grained motion (e.g., eyeball movement and lip motion).</p>
    <div class="row">
        <div class="col-md-12 col-sm-12 col-xs-12 gallery">
          <div style="position: relative; padding-top: 0; overflow: hidden;">
            <video controls muted style="display: block; width: 100%; height: auto;">
              <source src="src/video/ablation_phase1_facial_component_perceptual_loss.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>
    </div>
      <br>

      <div>
        <h2 class="text-center" style="margin-top: 30px;">
          Citation
        </h2>
        <p>
          If you want to cite our work, please use:
        </p>
        <pre>
          @article{ki2024float,
            title={FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait},
            author={Ki, Taekyung and Min, Dongchan and Chae, Gyeongsu},
            journal={arXiv preprint arXiv:2412.01064},
            year={2024}
          }

      </pre>
      </div>



      <div>
        <h2 class="text-center" style="margin-top: 30px;">
          Acknowledgement
        </h2>
        <p>
          The source images and audio are collected from the internet and other baselines, such as SadTalker, EMO, VASA-1, Hallo, LivePortrait, Loopy, and others.
          We appreciate their valuable contributions to this field.
          This project page is based on the project page of <a href="https://m-niemeyer.github.io/regnerf">RegNeRF</a>. You can easily use it from <a href="https://github.com/m-niemeyer/regnerf" target="_blank">the github repository</a>.
        </p>
      </div>
      <br>
      <br>
      <br>

      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
      </script>
      <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
        integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
        crossorigin="anonymous"></script>
      <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
        integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
        crossorigin="anonymous"></script>
        <script>
          const carousel = document.querySelector('#carouselExampleControls');
          const videos = carousel.querySelectorAll('video');
        
          videos.forEach(video => {
            video.addEventListener('play', () => {
              $(carousel).carousel('pause');
            });
        
            video.addEventListener('pause', () => {
              $(carousel).carousel('cycle');
            });
        
            video.addEventListener('ended', () => {
              $(carousel).carousel('cycle');
            });
          });
        </script>

        <script>
          const carousels = document.querySelectorAll('.carousel');
          
          carousels.forEach(carousel => {

            const videos = carousel.querySelectorAll('video');

            videos.forEach(video => {
              video.addEventListener('play', () => {
                $(carousel).carousel('pause');
              });
              video.addEventListener('pause', () => {
                $(carousel).carousel('cycle');
              });

              video.addEventListener('ended', () => {
                $(carousel).carousel('cycle');
              });
            });
          });
        </script>

      <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
      </script>

</body>

</html>
